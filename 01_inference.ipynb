{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b230c5-a636-4162-82c0-2218dc7733d8",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "\n",
    "## OpenAI Completion Request\n",
    "\n",
    "The Completions endpoint is typically used for base models. With the Completions endpoint, prompts are sent as plain strings, and the model produces the most likely text completions subject to the other parameters chosen. To stream the result, set `\"stream\": true`.\n",
    "\n",
    "You can use the [OpenAI Python API library](https://github.com/openai/openai-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607ac26-19ea-4926-a9e4-17290c62b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"not-used\")\n",
    "prompt = \"Once upon a time\"\n",
    "response = client.completions.create(\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=16,\n",
    "    stream=False\n",
    ")\n",
    "completion = response.choices[0].text\n",
    "print(completion)\n",
    "\n",
    "# Prints:\n",
    "# , there was a young man named Jack who lived in a small village at the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee737c31-faa9-47e5-831d-cbe738678e22",
   "metadata": {},
   "source": [
    "## OpenAI Chat Completion Request\n",
    "\n",
    "The Chat Completions endpoint is typically used with chat or instruct tuned models that are designed to be used through a conversational approach. With the Chat Completions endpoint, prompts are sent in the form of messages with roles and contents, giving a natural way to keep track of a multi-turn conversation. To stream the result, set `\"stream\": true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c3574-fd01-4e37-9a86-b15a88ecd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"not-used\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short limerick about the wonders of GPU computing.\"}\n",
    "]\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    messages=messages,\n",
    "    max_tokens=32,\n",
    "    stream=False\n",
    ")\n",
    "assistant_message = chat_response.choices[0].message\n",
    "print(assistant_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584197ab-2614-4126-a407-8c657cdd655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatCompletionMessage(content='There once was a GPU so fine,\\nProcessed data in parallel so divine,\\nIt crunched with great zest,\\nAnd computational quest,\\nUnleashing speed, a true wonder sublime!', role='assistant', function_call=None, tool_calls=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

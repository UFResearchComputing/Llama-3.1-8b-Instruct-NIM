{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b230c5-a636-4162-82c0-2218dc7733d8",
   "metadata": {},
   "source": [
    "# Run Inference\n",
    "\n",
    "## OpenAI Completion Request\n",
    "\n",
    "The Completions endpoint is typically used for base models. With the Completions endpoint, prompts are sent as plain strings, and the model produces the most likely text completions subject to the other parameters chosen. To stream the result, set `\"stream\": true`.\n",
    "\n",
    "You can use the [OpenAI Python API library](https://github.com/openai/openai-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2607ac26-19ea-4926-a9e4-17290c62b4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", in a small village nestled in the rolling hills of Tuscany, there\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"not-used\")\n",
    "prompt = \"Once upon a time\"\n",
    "response = client.completions.create(\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=16,\n",
    "    stream=False\n",
    ")\n",
    "completion = response.choices[0].text\n",
    "print(completion)\n",
    "\n",
    "# Prints:\n",
    "# , there was a young man named Jack who lived in a small village at the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee737c31-faa9-47e5-831d-cbe738678e22",
   "metadata": {},
   "source": [
    "## OpenAI Chat Completion Request\n",
    "\n",
    "The Chat Completions endpoint is typically used with chat or instruct tuned models that are designed to be used through a conversational approach. With the Chat Completions endpoint, prompts are sent in the form of messages with roles and contents, giving a natural way to keep track of a multi-turn conversation. To stream the result, set `\"stream\": true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601c3574-fd01-4e37-9a86-b15a88ecd5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='There once was a GPU so fine,\\nProcessed data in parallel divine,\\nIt crunched with great ease,\\nAlgorithmic freeze,\\nAnd results poured out in a short', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"not-used\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short limerick about the wonders of GPU computing.\"}\n",
    "]\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    messages=messages,\n",
    "    max_tokens=32,\n",
    "    stream=False\n",
    ")\n",
    "assistant_message = chat_response.choices[0].message\n",
    "print(assistant_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed4043f-a6cb-4e98-ba42-662634c250c9",
   "metadata": {},
   "source": [
    "If you want JSON-style output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "596446f5-39fb-4065-9f60-a9d391080712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant Response (JSON Format):\n",
      "{\n",
      "    \"content\": \"There once was a GPU so fine,\\nProcessed data in parallel divine,\\nIt crunched with great ease,\\nAlgorithmic freeze,\\nAnd results poured out in a short\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"function_call\": null,\n",
      "    \"tool_calls\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"not-used\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short limerick about the wonders of GPU computing.\"}\n",
    "]\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    messages=messages,\n",
    "    max_tokens=32,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "# Use .model_dump() instead of .dict() for Pydantic v2\n",
    "assistant_message = chat_response.choices[0].message.model_dump()\n",
    "\n",
    "print(\"\\nAssistant Response (JSON Format):\")\n",
    "print(json.dumps(assistant_message, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb7336-0ab6-43e4-a3b9-a1f1936fbd8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
